# 目标检测漏检问题的解决措施

**只有在 precison 得到保证的条件下讨论 recall 才有意义**

## 数据集平衡扩增

`干净且均衡的数据，从来都是解决问题最有力的办法。`对样本少的类别，进行样本扩增。扩增过程最好考虑新增样本的 diversity 以及 context。  
小目标扩增，复制粘贴：[参考链接](https://zhuanlan.zhihu.com/p/64635490)

## 改进网络结构

- 考虑样本少的类别的 bbox 尺寸、位置的分布先验，使生成的 region proposals 在这些类别有较高的召回（Recall）。
- 漏检很大程度上来自于目标尺度的巨大变化（车从远处开到近处）→image/feature pyramid(FPN)，multi-scale train/test 等。
  Trident Network 提出使用不同 ratio 的 dilated conv 分别匹配不同的感受野，达到检测不同尺度目标的目的：[参考链接](https://zhuanlan.zhihu.com/p/54334986)
- anchor 设置不合理，可以统计对 anchor 到三类：车，人，狗的 gt_bbox 之间的”命中率“。这个”命中率“可以是满足 IoU>0.5 时的 recall，也可以是 ATSS 方法中的 mean_IoU。漏检多的话”**命中率“应该显著比较低。（anchor 设置，feature map 的 stride）需要和 gt_bbox 在 scale 和 ratio 上匹配，检测效果才比较好。**所以依据这个原则来修正相关设置来提高上述的"命中率"。

## 改进损失函数

- Class-balanced Focal loss：对容易漏检的样本赋予较大的权重（Focal loss 对难分类的特征赋予较大权重）
- 易漏检样本通常在训练过程中较容易产生较大的 loss，可以在每次迭代中，将 loss 值较大的样本筛选出来加入训练集进行下一次迭代，让模型更多关注易漏检样本

## 后处理

- 改进 NMS：[目标检测之非极大值抑制(NMS)各种变体](https://zhuanlan.zhihu.com/p/50126479)

## lifelong learning

不同于 multi-task learning，lifelong learning 无需要求在每个 task 的训练过程中获取全部 task 的数据（所有类别的样本）。以经典的 Elastic Weights Consolidation (EWC)算法为例，可以根据各类样本分布，划分不同的子数据集得到不同的 detection tasks，训练完一个 task 后，在新的 task 的训练过程中，利用 EWC regularizer 调整 weights 的优化方向，使模型能够学习当前 detection task 的同时，不遗忘已经学习过的 detection tasks。这样最终得到的模型能够处理所有 task 中的所有类别的检测，对漏检问题应该有一定的改善作用。
****
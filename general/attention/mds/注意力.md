# 注意力

attention机制可以它认为是一种`资源分配的机制`，可以理解为`对于原本平均分配的资源根据attention对象的重要程度重新分配资源，重要的单位就多分一点，不重要或者不好的单位就少分一点`，在深度神经网络的结构设计中，attention所要分配的资源基本上就是`权重`了。  

视觉注意力分为几种，核心思想是基于原有的数据找到其之间的关联性，然后突出其某些重要特征，有`通道`注意力，`像素`注意力，`多阶`注意力等，也有把NLP中的`自注意力`引入。

## 自注意力（self-attention）

## 软注意力（soft-attention）

软注意力是一个[0,1]间的连续分布问题，更加关注区域或者通道，软注意力是确定性注意力，学习完成后可以通过网络生成，并且是``可微``的，可以``通过神经网络计算出梯度并且可以前向传播和后向反馈来学习得到注意力的权重``。  

### 空间域注意力 （spatial attention）

[论文地址](https://proceedings.neurips.cc/paper/2015/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html)  

[GitHub地址](https://github.com/fxia22/stn.pytorch)  

空间区域注意力可以理解为`让神经网络看哪里（关注位置）`。通过注意力机制，将原始图片中的空间信息变换到另一个空间中并保留了关键信息。例如：`spatial` transformer其实就是注意力机制的实现，因为训练出的spatial transformer能够`找出图片信息中需要被关注的区域`，同时这个transformer又能够具有`旋转、缩放变换`的功能，这样图片局部的重要信息能够通过变换而被框盒提取出来。  

[空间注意力](../imgs/1-空间注意力.jpg)  

[空间注意力](../imgs/2-空间注意力.jpg)  

[空间注意力](../imgs/3-空间注意力.jpg)  

[空间变换矩阵](../imgs/空间变换矩阵.png)  

空间变换矩阵代码：  
```python
class STN(Module):
    def __init__(self, layout = 'BHWD'):
        super(STN, self).__init__()
        if layout == 'BHWD':
            self.f = STNFunction()
        else:
            self.f = STNFunctionBCHW()
    def forward(self, input1, input2):
        return self.f(input1, input2)


class STNFunction(Function):
    def forward(self, input1, input2):
        self.input1 = input1
        self.input2 = input2
        self.device_c = ffi.new("int *")
        output = torch.zeros(input1.size()[0], input2.size()[1], input2.size()[2], input1.size()[3])
        #print('decice %d' % torch.cuda.current_device())
        if input1.is_cuda:
            self.device = torch.cuda.current_device()
        else:
            self.device = -1
        self.device_c[0] = self.device
        if not input1.is_cuda:
            my_lib.BilinearSamplerBHWD_updateOutput(input1, input2, output)
        else:
            output = output.cuda(self.device)
            my_lib.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output, self.device_c)
        return output

    def backward(self, grad_output):
        grad_input1 = torch.zeros(self.input1.size())
        grad_input2 = torch.zeros(self.input2.size())
        #print('backward decice %d' % self.device)
        if not grad_output.is_cuda:
            my_lib.BilinearSamplerBHWD_updateGradInput(self.input1, self.input2, grad_input1, grad_input2, grad_output)
        else:
            grad_input1 = grad_input1.cuda(self.device)
            grad_input2 = grad_input2.cuda(self.device)
            my_lib.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output, self.device_c)
        return grad_input1, grad_input2
```

### 通道注意力 （channel attention）

通道注意力可以理解为让神经网络在看什么，典型的代表是SENet。卷积网络的每一层都有很多卷积核，每个卷积核对应一个特征通道，相对于空间注意力机制，通道注意力在于分配各个卷积通道之间的资源，分配粒度比空间注意力机制大了一个级别（二维的面→三维的管）。  

[通道注意力](../imgs/通道注意力.jpg)  

[论文 Squeeze-and-Excitation Networks 地址](https://arxiv.org/abs/1709.01507)  

[GitHub地址](https://github.com/moskomule/senet.pytorch)  

Squeeze：将各个通道的全局空间特征作为该通道的表示，使用全局平均池化生成各通道的统计量。  

Excitation：学习各通道的依赖程度，并根据依赖程度对不同的特征图进行调整，得到最后的输出。  

整体的结构如图所示：  

[通道结构](../imgs/通道结构.jpg)  

卷积层的输出并没有考虑对各通道的依赖，SEBlock的目的在于网络选择性的增强信息量最大的特征，供后期处理充分利用这些特征并抑制无用的特征。  

SE-Inception Module
